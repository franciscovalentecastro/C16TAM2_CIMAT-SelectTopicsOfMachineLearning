{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'IroSvA2019/train/irosva.mx.training.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's examine what the data looks like.\n",
    "\n",
    "(Note: This repo does not contain the full data. To get the full data, go to the [Kaggle competition page](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) and download the data for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TOPIC</th>\n",
       "      <th>IS_IRONIC</th>\n",
       "      <th>MESSAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6424ee0864a0af40660686e135f5652b</td>\n",
       "      <td>asuntosConacyt</td>\n",
       "      <td>1</td>\n",
       "      <td>Rica económicamente, pero muy pobre en objetiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>f59978451dd7fb228830fed2ae00c3ef</td>\n",
       "      <td>asuntosConacyt</td>\n",
       "      <td>1</td>\n",
       "      <td>En algo tiene razón, mafias hay en todo, hasta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>280963c5eb0d162858caf3480a7ea08c</td>\n",
       "      <td>asuntosConacyt</td>\n",
       "      <td>1</td>\n",
       "      <td>¿De cuándo acá tan preocupados por la ciencia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>69af1d02743953e5a0e971cf6ac00c69</td>\n",
       "      <td>asuntosConacyt</td>\n",
       "      <td>1</td>\n",
       "      <td>De una vez que paren las titulaciones, que tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b0be044f4682eea74c6fe83a87fff22a</td>\n",
       "      <td>asuntosConacyt</td>\n",
       "      <td>1</td>\n",
       "      <td>@LopesDorigaa @Dolores_PL  Es el que también t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>bb3895deabd4aba3e328abdfa53323b4</td>\n",
       "      <td>asuntosConacyt</td>\n",
       "      <td>1</td>\n",
       "      <td>Pero ahí viene el joven prodigio y digno repre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>07e712aec9b92319cfff48e07d00731c</td>\n",
       "      <td>asuntosConacyt</td>\n",
       "      <td>1</td>\n",
       "      <td>@lopezobrador_   Habla de la Mafia del poder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>99baf109f3a25721ffdb30c529990b0a</td>\n",
       "      <td>asuntosConacyt</td>\n",
       "      <td>1</td>\n",
       "      <td>¿y la diseñadora de modas en el CONACYT?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5f70ec64d4cc6f4d628ce74008b815c5</td>\n",
       "      <td>asuntosConacyt</td>\n",
       "      <td>1</td>\n",
       "      <td>Esta 4T odia todo al que se tiene estudios. Na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>d8deeac521c1c8ad396f680f6b11db76</td>\n",
       "      <td>asuntosConacyt</td>\n",
       "      <td>1</td>\n",
       "      <td>Si es tan inteligente que lo manden a la Secre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID           TOPIC  IS_IRONIC  \\\n",
       "0  6424ee0864a0af40660686e135f5652b  asuntosConacyt          1   \n",
       "1  f59978451dd7fb228830fed2ae00c3ef  asuntosConacyt          1   \n",
       "2  280963c5eb0d162858caf3480a7ea08c  asuntosConacyt          1   \n",
       "3  69af1d02743953e5a0e971cf6ac00c69  asuntosConacyt          1   \n",
       "4  b0be044f4682eea74c6fe83a87fff22a  asuntosConacyt          1   \n",
       "5  bb3895deabd4aba3e328abdfa53323b4  asuntosConacyt          1   \n",
       "6  07e712aec9b92319cfff48e07d00731c  asuntosConacyt          1   \n",
       "7  99baf109f3a25721ffdb30c529990b0a  asuntosConacyt          1   \n",
       "8  5f70ec64d4cc6f4d628ce74008b815c5  asuntosConacyt          1   \n",
       "9  d8deeac521c1c8ad396f680f6b11db76  asuntosConacyt          1   \n",
       "\n",
       "                                             MESSAGE  \n",
       "0  Rica económicamente, pero muy pobre en objetiv...  \n",
       "1  En algo tiene razón, mafias hay en todo, hasta...  \n",
       "2  ¿De cuándo acá tan preocupados por la ciencia ...  \n",
       "3  De una vez que paren las titulaciones, que tod...  \n",
       "4  @LopesDorigaa @Dolores_PL  Es el que también t...  \n",
       "5  Pero ahí viene el joven prodigio y digno repre...  \n",
       "6  @lopezobrador_   Habla de la Mafia del poder y...  \n",
       "7           ¿y la diseñadora de modas en el CONACYT?  \n",
       "8  Esta 4T odia todo al que se tiene estudios. Na...  \n",
       "9  Si es tan inteligente que lo manden a la Secre...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(train_path).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Field class determines how the data is preprocessed and converted into a numeric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want comment_text field to be converted to lowercase, tokenized on whitespace, and preprocessed. So we tell that to the Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda x: x.split()\n",
    "TEXT = Field(sequential=True, tokenize=tokenize, lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was simple. The preprocessing of the labels is even easier, since they are already converted into a binary encoding.\n",
    "All we need to do is to tell the Field class that the labels are already processed. We do this by passing the use_vocab=False keyword to the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the TabularDataset class to read our data, since it is in csv format (TabularDataset handles csv, tsv, and json files as of now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the train and validation data, we need to process the labels. The fields we pass in must be in the same order as the columns. For fields we don't use, we pass in a tuple where the second element is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 106 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "              (\"topic\", TEXT),\n",
    "              (\"is_ironic\", LABEL),\n",
    "              (\"message\", TEXT)]\n",
    "\n",
    "trn = TabularDataset(path=train_path,\n",
    "        format='CSV',\n",
    "        skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "        fields=datafields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test data, we don't have any labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.dataset.TabularDataset at 0x1b935a0bc48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the TEXT field to convert words into integers, it needs to be told what the entire vocabulary is. To do this, we run TEXT.build_vocab, passing in the dataset to build the vocabulary on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 113 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEXT.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the vocab looks like.\n",
    "\n",
    "The vocab.freqs is a collections.Counter object, so we can take a look at the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('que', 2165),\n",
       " ('de', 1965),\n",
       " ('la', 1600),\n",
       " ('y', 1282),\n",
       " ('a', 1145),\n",
       " ('no', 1065),\n",
       " ('el', 1008),\n",
       " ('es', 943),\n",
       " ('en', 761),\n",
       " ('los', 655)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11809"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also instructive to take a look inside the Dataset. Datasets can be indexed like normal lists, so we'll look at the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.example.Example at 0x1b935a0be88>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of the dataset is an Example object that bundles the attributes of a single data point together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['topic', 'is_ironic', 'message'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the comment text is already tokenized for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rica', 'económicamente,', 'pero']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].message[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good. Now, let's build the Iterator which will allow us to load the data into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we'll be using a special kind of Iterator, called the **BucketIterator**.\n",
    "\n",
    "When we pass data into a neural network, we want the data to be padded to be the same length so that we can process them in batch:\n",
    "\n",
    "e.g.\n",
    "\\[ \n",
    "\\[3, 15, 2, 7\\],\n",
    "\\[4, 1\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] -> \\[ \n",
    "\\[3, 15, 2, 7, **0**\\],\n",
    "\\[4, 1, **0**, **0**, **0**\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] \n",
    "\n",
    "If the sequences differ greatly in length, the padding will consume a lot of wasteful memory and time.\n",
    "\n",
    "The BucketIterator groups sequences of similar lengths together for each batch to minimize padding. Handy, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "train_iter= BucketIterator(\n",
    "        trn, # we pass in the datasets we want the iterator to draw data from\n",
    "        batch_size=64,\n",
    "        device=0, # if you want to use the GPU, specify the GPU number here\n",
    "        sort_key=lambda x: len(x.message), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "        sort_within_batch=False,\n",
    "        repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the output of the BucketIterator looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 64]\n",
       "\t[.topic]:[torch.LongTensor of size 1x64]\n",
       "\t[.is_ironic]:[torch.LongTensor of size 64]\n",
       "\t[.message]:[torch.LongTensor of size 45x64]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__()); batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch has all the fields we passed to the Dataset as attributes. The batch data can be accessed through the attribute with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'fields', 'input_fields', 'target_fields', 'topic', 'is_ironic', 'message'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set, we don't want the data to be shuffled. This is why we'll be using a standard Iterator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping the Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the iterator returns a custom datatype called torchtext.data.Batch. This makes code reuse difficult (since each time the column names change, we need to modify the code), and makes torchtext hard to use with other libraries for some use cases (like torchsample and fastai). \n",
    "\n",
    "I hope this will be dealt with in the future (I'm considering filing a PR if I can decide what the API should look like), but in the meantime, we'll hack on a simple wrapper to make the batches easy to use. \n",
    "\n",
    "Concretely, we'll convert the batch to a tuple in the form (x, y) where x is the independent variable (the input to the model) and y is the dependent variable (the supervision data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # we pass in the list of attributes for x and y\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "            \n",
    "            if self.y_vars is not None: # we will concatenate y into a single tensor\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "\n",
    "            yield (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this to wrap the BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchWrapper(train_iter, \"message\", ['is_ironic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   8,  859,   10,  ...,   52, 7766,    4],\n",
       "         [  95, 1705, 7446,  ...,  151,   39,  266],\n",
       "         [8833,  220,   11,  ...,   82,   19,    7],\n",
       "         ...,\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1]]), tensor([[0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_dl.__iter__()); batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to start training a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Text Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a simple LSTM as a baseline example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBiLSTMBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=300,\n",
    "                 spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=1):\n",
    "        super().__init__() # don't forget to call this!\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1, dropout=recurrent_dropout)\n",
    "        self.linear_layers = []\n",
    "        for _ in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.predictor = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "        preds = self.predictor(feature)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleBiLSTMBaseline(\n",
       "  (embedding): Embedding(11809, 100)\n",
       "  (encoder): LSTM(100, 500, dropout=0.1)\n",
       "  (linear_layers): ModuleList()\n",
       "  (predictor): Linear(in_features=500, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_sz = 100\n",
    "nh = 500\n",
    "nl = 3\n",
    "model = SimpleBiLSTMBaseline(nh, emb_dim=em_sz); model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a GPU, remember to call model.cuda() to move your model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 38/38 [00:53<00:00,  1.41s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'valid_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_dl' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for x, y in tqdm.tqdm(train_dl): # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        opt.zero_grad()\n",
    "\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        print(loss.item())\n",
    "        \n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x, y in valid_dl:\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item() * x.size(0)\n",
    "\n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we output the data in the format required by the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-2350b9fe8955>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_dl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_dl' is not defined"
     ]
    }
   ],
   "source": [
    "test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for x, y in tqdm.tqdm(test_dl):\n",
    "    preds = model(x)\n",
    "    # if you're data is on the GPU, you need to move the data back to the cpu\n",
    "    # preds = preds.data.cpu().numpy()\n",
    "    preds = preds.data.numpy()\n",
    "    # the actual outputs of the model are logits, so we need to pass these values to the sigmoid function\n",
    "    preds = 1 / (1 + np.exp(-preds))\n",
    "    test_preds.append(preds)\n",
    "test_preds = np.hstack(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/test.csv\")\n",
    "for i, col in enumerate([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]):\n",
    "    df[col] = test_preds[:, i]\n",
    "\n",
    "# if you want to write the submission file to disk, uncomment and run the below code\n",
    "# df.drop(\"comment_text\", axis=1).to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
